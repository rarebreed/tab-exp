{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on transfprmers\n",
    "\n",
    "These are collected notes from several references including:\n",
    "\n",
    "- [How transformers work](https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34)\n",
    "- [Multi-head attention deep-dive](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)\n",
    "- [Tab Transformer Deep Dive](https://towardsdatascience.com/transformers-for-tabular-data-tabtransformer-deep-dive-5fb2438da820)\n",
    "- [FT-Transformer](https://towardsdatascience.com/transformers-for-tabular-data-tabtransformer-deep-dive-5fb2438da820)\n",
    "- [Multi-head attention pt 1](https://storrs.io/attention/)\n",
    "- [Multi-head attention pt 2](https://storrs.io/multihead-attention/)\n",
    "- [Manning: Build a LLM from Scratch](https://livebook.manning.com/book/build-a-large-language-model-from-scratch)\n",
    "- [Udemy: Transformers for NLP](https://www.udemy.com/course/data-science-transformers-nlp/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
