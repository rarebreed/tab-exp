{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up local AI code assist\n",
    "\n",
    "I have been using a local llama3 LLM with ollama and the continue vs code extension with great results so far.\n",
    "\n",
    "- install pixi\n",
    "- install ollama and llms\n",
    "- start ollama with llama3\n",
    "- install vs code extension\n",
    "- configure continue extension to use ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pixi\n",
    "\n",
    "We use pixi as a python manager, virtual environment, package manager and task runner all in one.\n",
    "Install it below.  If on windows, make sure you have winget installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from subprocess import call\n",
    "\n",
    "uname = platform.uname()\n",
    "if uname.system == 'Windows':\n",
    "   retcode = call([\"where\", \"pixi\"])\n",
    "   if retcode != 0:\n",
    "      print(\"installing pixi\")\n",
    "      !winget install prefix-dev.pixi\n",
    "   else:\n",
    "      print(\"pixi is installed\")\n",
    "else:\n",
    "   retcode = call([\"which\", \"pixi\"])\n",
    "   if retcode != 0:\n",
    "      print(\"installing pixi\")\n",
    "      !curl -fsSL https://pixi.sh/install.sh | bash\n",
    "   else:\n",
    "      print(\"pixi is installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install ollama and llms\n",
    "\n",
    "We already have some pixi tasks defined, so we will run them to install the ollama runner and\n",
    "the LLMs we will use for code assisstance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if uname.system == \"Windows\":\n",
    "    retcode = call([\"where\", \"ollama\"])\n",
    "    if retcode!= 0:\n",
    "        print(\"installing ollama\")\n",
    "        !winget install ollama\n",
    "    else:\n",
    "        print(\"ollama is installed\")\n",
    "else:\n",
    "    retcode = call([\"which\", \"ollama\"])\n",
    "    if retcode!= 0:\n",
    "        print(\"installing ollama\")\n",
    "        !pixi run ollama-install\n",
    "    else:\n",
    "        print(\"ollama is installed\")\n",
    "!pixi run ollama-pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start ollama with llama3\n",
    "\n",
    "We need to have ollama start llama3:8b so that the vscode extension can send messages to it.  Instead of running it in a cell in the notebook, run this in your shell instead\n",
    "\n",
    "```shell\n",
    "ollama run llama3:8b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install vs code extension\n",
    "\n",
    "Next we will install the vs code extension that allows us to hook into the LLM's we installed\n",
    "through ollama.  Again, we have a pixi task to help us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pixi run vscode-ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure continue extension \n",
    "\n",
    "Finally, we need to configure the continue extension so that it uses the ollama runner and the LLMs we installed above.\n",
    "\n",
    "Click on the gear icon in the bottom right corner of the view and select \"Configure\".  In the \n",
    "editor that pops up, enter the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"title\": \"Llama3-8b\",\n",
    "      \"model\": \"llama3:8b\",\n",
    "      \"contextLength\": 4096,\n",
    "      \"provider\": \"ollama\"\n",
    "    }\n",
    "  ],\n",
    "  \"tabAutoCompleteModel\": {\n",
    "    \"title\": \"Tab Complete Model\",\n",
    "    \"provider\": \"ollama\",\n",
    "    \"model\": \"starcoder2:3b\"\n",
    "  },\n",
    "  \"customCommands\": [\n",
    "    {\n",
    "      \"name\": \"test\",\n",
    "      \"prompt\": \"{{{ input }}}\\n\\nWrite a comprehensive set of unit tests for the selected code. It should setup, run tests that check for correctness including important edge cases, and teardown. Ensure that the tests are complete and sophisticated. Give the tests just as chat output, don't edit any file.\",\n",
    "      \"description\": \"Write unit tests for highlighted code\"\n",
    "    }\n",
    "  ],\n",
    "  \"allowAnonymousTelemetry\": false\n",
    "}\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
