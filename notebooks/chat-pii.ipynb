{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat to determine PII field\n",
    "\n",
    "This notebook will take a model checkpoint trained for sequence classification, and see how well it performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load library\n",
    "\n",
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from tab_exp.viz import model_choice, get_hf_model\n",
    "\n",
    "training, new_data, model_name = model_choice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an optimized model\n",
    "\n",
    "Create the model with the optimized config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import torch\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline, AutoTokenizer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str,\n",
    "        model_type: Literal[\"text_classifier\", \"causal\"],\n",
    "        train: bool,\n",
    "        init_prediction: bool = True\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.model_id = get_hf_model(model_name)\n",
    "        self.tokenizer = self.init_tokenizer()\n",
    "        self.terminators = [self.tokenizer.eos_token_id,\n",
    "                            self.tokenizer.convert_tokens_to_ids(\"\")]\n",
    "        quant_cfg = self.create_4bit_cfg()\n",
    "        match model_type:\n",
    "            case \"text_classifier\":\n",
    "                self.model = self.create_sequence_model(quant_cfg)\n",
    "            case _:\n",
    "                self.model = self.create_causal_model(quant_cfg)\n",
    "        self.config_model(train)\n",
    "        self.generator = pipeline(\"text-generation\", \n",
    "                                  model=self.model,\n",
    "                                  tokenizer=self.tokenizer,\n",
    "                                  )\n",
    "\n",
    "    def create_4bit_cfg(self) -> BitsAndBytesConfig:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise Exception(\"GPU must be available for training\")\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True, \n",
    "            bnb_4bit_quant_type = 'nf4',\n",
    "            bnb_4bit_use_double_quant = True, \n",
    "            bnb_4bit_compute_dtype = torch.bfloat16 \n",
    "        )\n",
    "        return quantization_config\n",
    "    \n",
    "    def _make_quant_cfg(self, quantization_cfg: BitsAndBytesConfig):\n",
    "        return dict(quantization_config=quantization_cfg,\n",
    "                    num_labels=4,\n",
    "                    device_map='auto')\n",
    "\n",
    "    def create_sequence_model(self, quant_cfg: BitsAndBytesConfig):\n",
    "        \"\"\"Creates a model used for text/sequence classification\n",
    "\n",
    "        Args:\n",
    "            quant_cfg (BitsAndBytesConfig): _description_\n",
    "\n",
    "        Returns:\n",
    "            AutoModelForSequenceClassification: the model\n",
    "        \"\"\"\n",
    "        print(f\"Using model {self.model_name}\")\n",
    "\n",
    "        # Create a model for text classification.  Normally llama3 is used for CausalLLM (question/answer)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            **self._make_quant_cfg(quant_cfg)\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def create_causal_model(self, quant_cfg: BitsAndBytesConfig):\n",
    "        \"\"\"Creates a model used for text generation/prompting\n",
    "\n",
    "        Args:\n",
    "            quant_cfg (BitsAndBytesConfig): _description_\n",
    "\n",
    "        Returns:\n",
    "            AutoModelForCausalLM: the model\n",
    "        \"\"\"\n",
    "        print(f\"Using model {self.model_name}\")\n",
    "\n",
    "        # Create a model for chat prompting. \n",
    "        return AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            **self._make_quant_cfg(quant_cfg)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def init_tokenizer(self) -> PreTrainedTokenizer | PreTrainedTokenizerFast:\n",
    "        \"\"\"Retrieves and configures tokenizer based on the model\n",
    "\n",
    "        Returns:\n",
    "            PreTrainedTokenizer | PreTrainedTokenizerFast: _description_\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        if \"llama3\" in self.model_id or \"Llama-3\" in self.model_id:\n",
    "            # The llama3 tokenizer doesn't do padding like other models.  So set them as End of Sequence\n",
    "            print(\"Setting tokenizer for llama3\")\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        print('initialized tokenizer')\n",
    "        return tokenizer\n",
    "    \n",
    "    def lora_config(self) -> LoraConfig:\n",
    "        \"\"\"LoRA configuration to train only needed weights in the model\n",
    "\n",
    "        Returns:\n",
    "            LoraConfig:\n",
    "        \"\"\"\n",
    "        return LoraConfig(\n",
    "            r = 16, \n",
    "            lora_alpha = 8,\n",
    "            target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "            lora_dropout = 0.05, \n",
    "            bias = 'none',\n",
    "            task_type = 'SEQ_CLS'\n",
    "        )\n",
    "    \n",
    "    def config_model(self, training: bool):\n",
    "        # The model is now optimized to make training faster, if a little less accurate\n",
    "        if training:\n",
    "            print(\"Configuring model for training\")\n",
    "            self.model = prepare_model_for_kbit_training(self.model)\n",
    "            self.model = get_peft_model(self.model, self.lora_config())\n",
    "            # set some llama3 tokenizer specific settings\n",
    "            self.model.config.use_cache = False  # type: ignore\n",
    "            self.model.config.pretraining_tp = 1 # type: ignore\n",
    "        else:\n",
    "            print(\"Using checkpointed model to get predictions\")\n",
    "\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id  # type: ignore\n",
    "  \n",
    "    def get_response(\n",
    "          self, \n",
    "          query: str, \n",
    "          message_history: list[dict] | None = None, \n",
    "          max_tokens=1024*124, \n",
    "          temperature=0.6, \n",
    "          top_p=0.9\n",
    "      ):\n",
    "        if message_history is None:\n",
    "            message_history = []\n",
    "        user_prompt = message_history + [{\"role\": \"user\", \"content\": query}]\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            user_prompt, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        print(prompt)\n",
    "\n",
    "        outputs = self.generator(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            eos_token_id=self.terminators,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        print(outputs)\n",
    "        response = outputs[0][\"generated_text\"][len(prompt):] # type: ignore\n",
    "        return response, user_prompt + [{\"role\": \"assistant\", \"content\": response}]\n",
    "    \n",
    "    def chatbot(self, system_instructions=\"\"):\n",
    "        conversation = [{\"role\": \"system\", \"content\": system_instructions}]\n",
    "        # self.generator = pipeline(\n",
    "        #     \"text-generation\",\n",
    "        #     model=self.model_id,\n",
    "        #     model_kwargs={\n",
    "        #         \"torch_dtype\": torch.float16,\n",
    "        #         \"quantization_config\": {\"load_in_4bit\": True},\n",
    "        #         \"low_cpu_mem_usage\": True,\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"User: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"Exiting the chatbot. Goodbye!\")\n",
    "                break\n",
    "            response, conversation = self.get_response(user_input, conversation)\n",
    "            print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = str(model_name.value)\n",
    "selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3 = Llama3(model_name=str(model_name.value),\n",
    "                model_type=\"causal\",\n",
    "                train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3.chatbot(\"You are an expert in data engineering and work with General Data Protection Regulation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
