{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classification for anonymization\n",
    "\n",
    "This notebook is an experiment to classify key-value fields from JSON as being anonymized or not for PII data.\n",
    "\n",
    "The first step is to create our model name and get a tokenizer to tokenize our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Unfortunately, it appears huggingfaces doesn't fully support 3.1 for text classification in the \n",
    "# AutoModelForSequenceClassification yet.  So we will just use 3.0 for now\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# The llama3 tokenizer doesn't do padding like other models.  So set them as End of Sequence\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import tab_exp module\n",
    "\n",
    "We need the tab_exp module to generate the synthetic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from tab_exp.tab import generate_synth_data, PIIData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset\n",
    "\n",
    "We will use the generate_synth_data to create datasets for training, testing, and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "sd_train = generate_synth_data(samples=1000, output=\"samples_train\", clean=True)\n",
    "ds_train = load_dataset(\"json\", data_files=sd_train[\"combined_path\"])\n",
    "\n",
    "sd_test = generate_synth_data(samples=300, output=\"samples_test\", clean=True)\n",
    "ds_test = load_dataset(\"json\", data_files=sd_test[\"combined_path\"])\n",
    "\n",
    "sd_validate = generate_synth_data(samples=200, output=\"samples_validate\", clean=True)\n",
    "ds_validate = load_dataset(\"json\", data_files=sd_validate[\"combined_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": ds_train[\"train\"],\n",
    "    \"validate\": ds_validate[\"train\"],\n",
    "    \"test\": ds_test[\"train\"]\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize for efficiency\n",
    "\n",
    "For experimentation on a small computer, we need to quantize the weights to make them smaller to trade accuracy for\n",
    "computational speed.  Without this, we either will not be able to fit the model weights at all into memory, or it will\n",
    "take forever to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"GPU must be available for trainin\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 \n",
    ")\n",
    "\n",
    "# Create a model for text classification.  Normally llama3 is used for CausalLLM (question/answer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=4,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LoRA to train only a subset of the weights\n",
    "\n",
    "Fine tuning all the weights of the checkpoint would be too prohibitive.  So we will use LoRA to train only a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no idea why the jupyter notebook lsp says it can't find the symbol for these exports.  This will work\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model # type: ignore\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "# The model is now optimized to make training faster, if a little less accurate\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "# set some llama3 tokenizer specific settings\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl \n",
    "\n",
    "df = pl.read_ndjson(sd_train[\"combined_path\"])\n",
    "df = df.to_pandas()\n",
    "df = df[:8000]\n",
    "sentences = df.text.to_list()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a sample run to get tensors\n",
    "\n",
    "We load the raw text data into a list of str and then batched up into sublists.  The sublists are then passed to the \n",
    "tokenize function so that it is in the format that pytorch can use.\n",
    "\n",
    "This `inputs` value is then passed to the model to calculate the logits for each sentence and stored in `all_outputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "accel = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for i in range(0, len(sentences), batch_size):\n",
    "    batched_inputs = sentences[i:i + batch_size]\n",
    "    \n",
    "    inputs = tokenizer(batched_inputs, truncation=True, padding=True, return_tensors=\"pt\", max_length=512)\n",
    "    inputs = {k: v.to(accel) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        all_outputs.append(outputs['logits'])\n",
    "    print(i)\n",
    "\n",
    "\n",
    "final_output = torch.cat(all_outputs, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predictions'] = final_output.argmax(dim=1).cpu().numpy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance\n",
    "\n",
    "We need a way to evaluate the performance.  By default HF will use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "def get_metrics_result(test_df: pd.DataFrame):\n",
    "    y_test = test_df.label\n",
    "    y_pred = test_df.predictions\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "get_metrics_result(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the dataset\n",
    "\n",
    "We need to convert the natural language in the dataset to the embeddings needed by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# function that will be applied to the testing data.  We need to tokenize it for training\n",
    "def tokenize_fn(data: PIIData):\n",
    "    return tokenizer(data['text'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_ds.set_format(\"torch\")\n",
    "\n",
    "# pad the batch of inputs to a length equal to the maximum input length in that batch\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom trainer\n",
    "\n",
    "We will create a custom trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(evaluations):\n",
    "    predictions, labels = evaluations\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),\n",
    "    'accuracy':accuracy_score(predictions,labels)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'sentiment_classification',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 1,\n",
    "    logging_steps=1,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_ds['train'],\n",
    "    eval_dataset = tokenized_ds['validate'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.peft_model import PeftModel\n",
    "from peft.mixed_model import PeftMixedModel\n",
    "\n",
    "def generate_predictions(\n",
    "    model: PeftModel | PeftMixedModel, \n",
    "    df_test: pd.DataFrame,\n",
    "    batch_sz: int = 32\n",
    "):\n",
    "    sentences = df_test.text.tolist()\n",
    "    batch_size = batch_sz\n",
    "    all_outputs = []\n",
    "    accel = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "        inputs = tokenizer(batch_sentences, \n",
    "                           return_tensors=\"pt\", \n",
    "                           padding=True, \n",
    "                           truncation=True, \n",
    "                           max_length=512)\n",
    "\n",
    "        inputs = {k: v.to(accel) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            all_outputs.append(outputs['logits'])\n",
    "        print(i)\n",
    "        \n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    df_test['predictions'] = final_outputs.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "#test_df = df[1000:2000]\n",
    "generate_predictions(model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_width_chars(200)\n",
    "pl.Config.set_fmt_str_lengths(80)\n",
    "pl.from_pandas(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
