{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classification for anonymization\n",
    "\n",
    "This notebook is an experiment to classify key-value fields from JSON as being anonymized or not for PII data.\n",
    "\n",
    "First, export functions to generate training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from tab_exp.tab import generate_synth_data, PIIData\n",
    "from tab_exp.viz import model_choice, dataset_choice, get_hf_model\n",
    "\n",
    "do_train, new_data, model_name = model_choice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Tokenizer \n",
    "\n",
    "The first step is to create our model name and get a tokenizer to tokenize our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "selected_model = str(model_name.value)\n",
    "use_checkpoint = True if \"finetuned\" in selected_model else False\n",
    "print(f\"Using checkpoint = {use_checkpoint}\")\n",
    "\n",
    "model_id = get_hf_model(selected_model)\n",
    "print(f\"Using model {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# The llama3 tokenizer doesn't do padding like other models.  So set them as End of Sequence\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset\n",
    "\n",
    "We will use the generate_synth_data to create datasets for training, testing, and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_p(p: str): return f\"../notebooks/samples_{p}/combined/combined.jsonl\"\n",
    "\n",
    "if new_data.value:\n",
    "    sd_train = generate_synth_data(samples=1000, output=\"samples_train\", clean=True)\n",
    "    train_path = sd_train[\"combined_path\"]\n",
    "    sd_test = generate_synth_data(samples=300, output=\"samples_test\", clean=True)\n",
    "    test_path = sd_test[\"combined_path\"]\n",
    "    sd_validate = generate_synth_data(samples=200, output=\"samples_validate\", clean=True)\n",
    "    validate_path = sd_validate[\"combined_path\"]\n",
    "else:\n",
    "    train_path = data_p(\"train\")\n",
    "    test_path = data_p(\"test\")\n",
    "    validate_path = data_p(\"validate\")\n",
    "\n",
    "ds_train = load_dataset(\"json\", data_files=train_path)\n",
    "ds_test = load_dataset(\"json\", data_files=test_path)\n",
    "ds_validate = load_dataset(\"json\", data_files=validate_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "\n",
    "ds_train: Dataset = cast(Dataset, ds_train)\n",
    "ds_validate: Dataset = cast(Dataset, ds_validate)\n",
    "ds_test: Dataset = cast(Dataset, ds_test)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": ds_train[\"train\"],\n",
    "    \"validate\": ds_validate[\"train\"],\n",
    "    \"test\": ds_test[\"train\"]\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe of test data\n",
    "\n",
    "Create the dataframe to make it more convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_to_use = dataset_choice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dataset = str(ds_to_use.value)\n",
    "print(f\"Using dataset {use_dataset}\")\n",
    "pl_df = pl.read_ndjson(data_p(use_dataset))\n",
    "\n",
    "df = pl_df[:8000].clone().to_pandas()\n",
    "# df2 = pd_df[8000:16000]\n",
    "sentences = df.text.to_list()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize for efficiency\n",
    "\n",
    "For experimentation on a small computer, we need to quantize the weights to make them smaller to trade accuracy for\n",
    "computational speed.  Without this, we either will not be able to fit the model weights at all into memory, or it will\n",
    "take forever to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"GPU must be available for training\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an optimized model for finetuning\n",
    "\n",
    "Create an optimized model that we can use for fine tuning on small hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using model {selected_model}\")\n",
    "\n",
    "# Create a model for text classification.  Normally llama3 is used for CausalLLM (question/answer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    selected_model,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=4,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LoRA to train only a subset of the weights\n",
    "\n",
    "Fine tuning all the weights of the checkpoint would be too prohibitive.  So we will use LoRA to train only a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no idea why the jupyter notebook lsp says it can't find the symbol for these exports.  This will work\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model # type: ignore\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    task_type = 'SEQ_CLS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is now optimized to make training faster, if a little less accurate\n",
    "def init_model_for_training(model, train: bool):\n",
    "    if train:\n",
    "        print(\"Configuring model for training\")\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        # set some llama3 tokenizer specific settings\n",
    "        model.config.use_cache = False  # type: ignore\n",
    "        model.config.pretraining_tp = 1 # type: ignore\n",
    "    else:\n",
    "        print(\"Using checkpointed model to get predictions\")\n",
    "\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model_for_training(model, do_train.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make initial prediction\n",
    "\n",
    "Do a sample run to get tensors\n",
    "\n",
    "We load the raw text data into a list of str and then batched up into sublists.  The sublists are then passed to the \n",
    "tokenize function so that it is in the format that pytorch can use.\n",
    "\n",
    "This `inputs` value is then passed to the model to calculate the logits for each sentence and stored in `all_outputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.peft_model import PeftModel\n",
    "from peft.mixed_model import PeftMixedModel\n",
    "import tqdm\n",
    "\n",
    "def generate_predictions(model: PeftModel | PeftMixedModel, df_test: pd.DataFrame, batch_sz: int = 32):\n",
    "    sentences = df_test.text.tolist()\n",
    "    all_outputs = []\n",
    "    accel = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    with tqdm.tqdm(total=len(sentences)) as pbar:\n",
    "        for i in range(0, len(sentences), batch_sz):\n",
    "            batch_sentences = sentences[i:i + batch_sz]\n",
    "            # Encode the sentcences with the tokenizer.  Each LLM has its own tokenizer and config\n",
    "            inputs = tokenizer(batch_sentences, \n",
    "                               return_tensors=\"pt\", \n",
    "                               padding=True, truncation=True, \n",
    "                               max_length=512)\n",
    "            inputs = {k: v.to(accel) for k, v in inputs.items()}\n",
    "            # Don't actually do backprop with grad descent.  We just want a prediction\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                all_outputs.append(outputs['logits'])\n",
    "            pbar.update(batch_sz)\n",
    "    return torch.cat(all_outputs, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predictions column\n",
    "\n",
    "Generate predictions and add a predictions column to the test data datafrane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generate_predictions(model, df)\n",
    "df[\"predictions\"] = outputs.argmax(dim=1).cpu().numpy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance\n",
    "\n",
    "We need a way to evaluate the performance.  By default HF will use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "\n",
    "def get_metrics_result(test_df: pd.DataFrame):\n",
    "    y_test = test_df.label\n",
    "    y_pred = test_df.predictions\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "    return {\"classification_report\": classification_report(y_test, y_pred),\n",
    "            \"balanced_accuracy_score\": balanced_accuracy_score(y_test, y_pred),\n",
    "            \"accuracy_score\": accuracy_score(y_test, y_pred)}\n",
    "\n",
    "def compute_metrics(evaluations):\n",
    "    predictions, labels = evaluations\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'balanced_accuracy': balanced_accuracy_score(predictions, labels),\n",
    "            'accuracy':accuracy_score(predictions,labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the metrics\n",
    "\n",
    "Calculate metrics with the stock LLM before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_result(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the dataset\n",
    "\n",
    "We need to convert the natural language in the dataset to the embeddings needed by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# function that will be applied to the testing data.  We need to tokenize it for training\n",
    "def tokenize_fn(data: PIIData):\n",
    "    return tokenizer(data['text'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_ds.set_format(\"torch\")\n",
    "\n",
    "# pad the batch of inputs to a length equal to the maximum input length in that batch\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom trainer\n",
    "\n",
    "We will create a custom trainer.  Importantly, we define a way to calculate loss for back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = (torch.tensor(class_weights, dtype=torch.float32)\n",
    "                                  .to(self.args.device))\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = 'sentiment_classification',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 1,\n",
    "    logging_steps=1,\n",
    "    weight_decay = 0.01,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_ds['train'],\n",
    "    eval_dataset = tokenized_ds['validate'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training\n",
    "\n",
    "Actually fine tune the model by training it on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tab_exp.viz import get_finetuned_name\n",
    "finetuned_name = get_finetuned_name(selected_model)\n",
    "print(f\"Saving checkpoint as {finetuned_name}\")\n",
    "model.save_pretrained(finetuned_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new predictions\n",
    "\n",
    "Now that we have a newly trained model, run new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2 = generate_predictions(model, df)\n",
    "df[\"predictions\"] = outputs2.argmax(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_width_chars(200)\n",
    "pl.Config.set_fmt_str_lengths(80)\n",
    "pl_df = pl.from_pandas(df)\n",
    "pl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate new metrics with fine tuned model\n",
    "\n",
    "Now that we have the trained weight checkpoint, run the  \n",
    "get_metrics_result again and see what the performance is like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_result(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
