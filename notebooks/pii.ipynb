{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classification for anonymization\n",
    "\n",
    "This notebook is an experiment to classify key-value fields from JSON as being anonymized or not for PII data.\n",
    "\n",
    "The first step is to create our model name and get a tokenizer to tokenize our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as iw\n",
    "\n",
    "saved = iw.Checkbox(\n",
    "    value=False,\n",
    "    description='Use Saved',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "initial = iw.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Initial Predictions\",\n",
    "    indent=False\n",
    ")\n",
    "iw.VBox([saved, initial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "use_checkpoint = saved.value\n",
    "print(f\"Using checkpoint = {use_checkpoint}\")\n",
    "\n",
    "# Unfortunately, it appears huggingfaces doesn't fully support 3.1 for text classification in the \n",
    "# AutoModelForSequenceClassification yet.  So we will just use 3.0 for now\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# The llama3 tokenizer doesn't do padding like other models.  So set them as End of Sequence\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import tab_exp module\n",
    "\n",
    "We need the tab_exp module to generate the synthetic test data\n",
    "\n",
    "### When to run\n",
    "\n",
    "This should always be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from tab_exp.tab import generate_synth_data, PIIData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset\n",
    "\n",
    "We will use the generate_synth_data to create datasets for training, testing, and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_train = generate_synth_data(samples=1000, output=\"samples_train\", clean=True)\n",
    "ds_train = load_dataset(\"json\", data_files=sd_train[\"combined_path\"])\n",
    "\n",
    "sd_test = generate_synth_data(samples=300, output=\"samples_test\", clean=True)\n",
    "ds_test = load_dataset(\"json\", data_files=sd_test[\"combined_path\"])\n",
    "\n",
    "sd_validate = generate_synth_data(samples=200, output=\"samples_validate\", clean=True)\n",
    "ds_validate = load_dataset(\"json\", data_files=sd_validate[\"combined_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "\n",
    "ds_train: Dataset = cast(Dataset, ds_train)\n",
    "ds_validate: Dataset = cast(Dataset, ds_validate)\n",
    "ds_test: Dataset = cast(Dataset, ds_test)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": ds_train[\"train\"],\n",
    "    \"validate\": ds_validate[\"train\"],\n",
    "    \"test\": ds_test[\"train\"]\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe of test data\n",
    "\n",
    "Create the dataframe to make it more convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"..\\\\notebooks\\\\samples_train\\\\combined\\\\combined.jsonl\"\n",
    "try:\n",
    "    pl_df = pl.read_ndjson(sd_train[\"combined_path\"])\n",
    "except NameError:\n",
    "    pl_df = pl.read_ndjson(path)\n",
    "df = pl_df[:8000].clone().to_pandas()\n",
    "# pd_df: pd.DataFrame = pl_df.to_pandas()\n",
    "# df = pd_df[:8000]\n",
    "# df2 = pd_df[8000:16000]\n",
    "sentences = df.text.to_list()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize for efficiency\n",
    "\n",
    "For experimentation on a small computer, we need to quantize the weights to make them smaller to trade accuracy for\n",
    "computational speed.  Without this, we either will not be able to fit the model weights at all into memory, or it will\n",
    "take forever to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"GPU must be available for training\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a fine-tuning optimized model\n",
    "\n",
    "Create an optimized model that we can use for fine tuning on small hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "if use_checkpoint:\n",
    "    model_checkpoint = \"finetuned-v2\"\n",
    "else:\n",
    "    model_checkpoint = model_id\n",
    "\n",
    "print(f\"Using model {model_checkpoint}\")\n",
    "\n",
    "# Create a model for text classification.  Normally llama3 is used for CausalLLM (question/answer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=4,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LoRA to train only a subset of the weights\n",
    "\n",
    "Fine tuning all the weights of the checkpoint would be too prohibitive.  So we will use LoRA to train only a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no idea why the jupyter notebook lsp says it can't find the symbol for these exports.  This will work\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model # type: ignore\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    task_type = 'SEQ_CLS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is now optimized to make training faster, if a little less accurate\n",
    "if not initial.value:\n",
    "    print(\"Configuring model for training\")\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    # set some llama3 tokenizer specific settings\n",
    "    model.config.use_cache = False  # type: ignore\n",
    "    model.config.pretraining_tp = 1 # type: ignore\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a sample run to get tensors\n",
    "\n",
    "We load the raw text data into a list of str and then batched up into sublists.  The sublists are then passed to the \n",
    "tokenize function so that it is in the format that pytorch can use.\n",
    "\n",
    "This `inputs` value is then passed to the model to calculate the logits for each sentence and stored in `all_outputs`.\n",
    "\n",
    "### When to run\n",
    "\n",
    "This should always be run, whether for new training, or from a loaded checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.peft_model import PeftModel\n",
    "from peft.mixed_model import PeftMixedModel\n",
    "import tqdm\n",
    "\n",
    "def generate_predictions(\n",
    "    model: PeftModel | PeftMixedModel, \n",
    "    df_test: pd.DataFrame,\n",
    "    batch_sz: int = 32\n",
    "):\n",
    "    sentences = df_test.text.tolist()\n",
    "    batch_size = batch_sz\n",
    "    all_outputs = []\n",
    "    accel = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    with tqdm.tqdm(total=len(sentences)) as pbar:\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "            inputs = tokenizer(batch_sentences, \n",
    "                            return_tensors=\"pt\", \n",
    "                            padding=True, \n",
    "                            truncation=True, \n",
    "                            max_length=512)\n",
    "\n",
    "            inputs = {k: v.to(accel) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                all_outputs.append(outputs['logits'])\n",
    "            pbar.update(batch_size)\n",
    "            \n",
    "        \n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    return final_outputs\n",
    "    #df_test['predictions'] = final_outputs.argmax(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generate_predictions(model, df)\n",
    "df[\"predictions\"] = outputs.argmax(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance\n",
    "\n",
    "We need a way to evaluate the performance.  By default HF will use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def get_metrics_result(test_df: pd.DataFrame):\n",
    "    y_test = test_df.label\n",
    "    y_pred = test_df.predictions\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "def compute_metrics(evaluations):\n",
    "    predictions, labels = evaluations\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'balanced_accuracy': balanced_accuracy_score(predictions, labels),\n",
    "            'accuracy':accuracy_score(predictions,labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_result(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the dataset\n",
    "\n",
    "We need to convert the natural language in the dataset to the embeddings needed by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# function that will be applied to the testing data.  We need to tokenize it for training\n",
    "def tokenize_fn(data: PIIData):\n",
    "    return tokenizer(data['text'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_ds.set_format(\"torch\")\n",
    "\n",
    "# pad the batch of inputs to a length equal to the maximum input length in that batch\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom trainer\n",
    "\n",
    "We will create a custom trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = 'sentiment_classification',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 1,\n",
    "    logging_steps=1,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_ds['train'],\n",
    "    eval_dataset = tokenized_ds['validate'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training\n",
    "\n",
    "Actually fine tune the model by training it on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"finetuned-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved checkpoint\n",
    "\n",
    "Load weights from a previously saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"finetuned-v2\")\n",
    "\n",
    "# Create a model for text classification.  Normally llama3 is used for CausalLLM (question/answer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"finetuned-v2\",\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=4,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new predictions\n",
    "\n",
    "Now that we have a newly trained model, run new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_predictions(model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_width_chars(200)\n",
    "pl.Config.set_fmt_str_lengths(80)\n",
    "pl_df = pl.from_pandas(df)\n",
    "pl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate new metrics with fine tuned model\n",
    "\n",
    "Now that we have the trained weight checkpoint, run the get_metrics_result again and see what the performance is like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_result(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
