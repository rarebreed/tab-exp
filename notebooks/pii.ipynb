{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classification for anonymization\n",
    "\n",
    "This notebook is an experiment to classify key-value fields from JSON as being anonymized or not for PII data.\n",
    "\n",
    "The first step is to create our model name and get a tokenizer to tokenize our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# The llama3 tokenizer doesn't do padding like other models.  So set them as End of Sequence\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import tab_exp module\n",
    "\n",
    "We need the tab_exp module to generate the synthetic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from tab_exp.tab import generate_synth_data, PIIData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset\n",
    "\n",
    "We will use the generate_synth_data to create datasets for training, testing, and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "sd_train = generate_synth_data(samples=1000, output=\"samples_train\", clean=True)\n",
    "ds_train = load_dataset(\"json\", data_files=sd_train[\"combined_path\"], split=\"train\")\n",
    "\n",
    "sd_test = generate_synth_data(samples=300, output=\"samples_test\", clean=True)\n",
    "ds_test = load_dataset(\"json\", data_files=sd_test[\"combined_path\"])\n",
    "\n",
    "sd_validate = generate_synth_data(samples=100, output=\"samples_validate\", clean=True)\n",
    "ds_validate = load_dataset(\"json\", data_files=sd_validate[\"combined_path\"])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": ds_train,\n",
    "    \"validate\": ds_validate,\n",
    "    \"test\": ds_test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize for efficiency\n",
    "\n",
    "For experimentation on a small computer, we need to quantize the weights to make them smaller to trade accuracy for\n",
    "computational speed.  Without this, we either will not be able to fit the model weights at all into memory, or it will\n",
    "take forever to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"GPU must be available for trainin\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 \n",
    ")\n",
    "# Create a model for text classification.  Normally llama3 is used for CausalLLM (question/answer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=4,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LoRA to train only a subset of the weights\n",
    "\n",
    "Fine tuning all the weights of the checkpoint would be too prohibitive.  So we will use LoRA to train only a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the dataset\n",
    "\n",
    "We need to convert the natural language in the dataset to the embeddings needed by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# function that will be applied to the testing data.  We need to tokenize it for training\n",
    "def tokenize_fn(data: PIIData):\n",
    "    return tokenizer(data['text'], truncation=True, padding=True)\n",
    "\n",
    "tokenized_train_ds: Dataset = cast(Dataset, ds_train.map(tokenize_fn, batched=True, remove_columns=[\"text\"]))\n",
    "tokenized_train_ds.set_format(\"torch\")\n",
    "# pad the batch of inputs to a length equal to the maximum input length in that batch\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(sd_train[\"combined_path\"], \"r\") as cf:\n",
    "    text: list[str] = [json.loads(line)[\"text\"] for line in cf.readlines()]\n",
    "\n",
    "text = text[:128]\n",
    "\n",
    "accel = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for i in range(0, len(text), batch_size):\n",
    "    batched_inputs = text[i:i + batch_size]\n",
    "    \n",
    "    inputs = tokenizer(batched_inputs, truncation=True, padding=True, return_tensors=\"pt\", max_length=512)\n",
    "    inputs = {k: v.to(accel) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        all_outputs.append(outputs['logits'])\n",
    "    print(all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance\n",
    "\n",
    "We need a way to evaluate the performance.  By default HF will use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "\n",
    "def get_metrics_result(test_df):\n",
    "    y_test = test_df.label\n",
    "    y_pred = test_df.predictions\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "def compute_metrics(evaluations):\n",
    "    predictions, labels = evaluations\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),\n",
    "    'accuracy':accuracy_score(predictions,labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom trainer\n",
    "\n",
    "We will create a custom trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
